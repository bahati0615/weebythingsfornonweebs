{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p8QRrhMnjKoH"
   },
   "source": [
    "From the 3 CSV files, we will begin cleaing and filtering the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "id": "_BzZFlG_ioBA",
    "outputId": "4f2552f1-ceea-471b-c2d3-462c8c3c9283"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 1000000 rows...\n",
      "processed 2000000 rows...\n",
      "processed 3000000 rows...\n",
      "processed 4000000 rows...\n",
      "processed 5000000 rows...\n",
      "processed 6000000 rows...\n",
      "processed 7000000 rows...\n",
      "processed 8000000 rows...\n",
      "processed 9000000 rows...\n",
      "processed 10000000 rows...\n",
      "processed 11000000 rows...\n",
      "processed 12000000 rows...\n",
      "processed 13000000 rows...\n",
      "processed 14000000 rows...\n",
      "processed 15000000 rows...\n",
      "processed 16000000 rows...\n",
      "processed 17000000 rows...\n",
      "processed 18000000 rows...\n",
      "processed 19000000 rows...\n",
      "processed 20000000 rows...\n",
      "processed 21000000 rows...\n",
      "processed 22000000 rows...\n",
      "processed 23000000 rows...\n",
      "processed 24000000 rows...\n",
      "processed 25000000 rows...\n",
      "processed 26000000 rows...\n",
      "processed 27000000 rows...\n",
      "processed 28000000 rows...\n",
      "processed 29000000 rows...\n",
      "processed 30000000 rows...\n",
      "processed 31000000 rows...\n",
      "processed 32000000 rows...\n",
      "processed 33000000 rows...\n",
      "processed 34000000 rows...\n",
      "processed 35000000 rows...\n",
      "processed 36000000 rows...\n",
      "processed 37000000 rows...\n",
      "processed 38000000 rows...\n",
      "processed 39000000 rows...\n",
      "processed 40000000 rows...\n",
      "processed 41000000 rows...\n",
      "processed 42000000 rows...\n",
      "processed 43000000 rows...\n",
      "processed 44000000 rows...\n",
      "processed 45000000 rows...\n",
      "processed 46000000 rows...\n",
      "processed 47000000 rows...\n",
      "processed 48000000 rows...\n",
      "processed 49000000 rows...\n",
      "processed 50000000 rows...\n",
      "processed 51000000 rows...\n",
      "processed 52000000 rows...\n",
      "processed 53000000 rows...\n",
      "processed 54000000 rows...\n",
      "processed 55000000 rows...\n",
      "processed 56000000 rows...\n",
      "processed 57000000 rows...\n",
      "processed 58000000 rows...\n",
      "processed 59000000 rows...\n",
      "processed 60000000 rows...\n",
      "processed 61000000 rows...\n",
      "processed 62000000 rows...\n",
      "processed 63000000 rows...\n",
      "processed 64000000 rows...\n",
      "processed 65000000 rows...\n",
      "processed 66000000 rows...\n",
      "processed 67000000 rows...\n",
      "processed 68000000 rows...\n",
      "processed 69000000 rows...\n",
      "processed 70000000 rows...\n",
      "processed 71000000 rows...\n",
      "processed 72000000 rows...\n",
      "processed 73000000 rows...\n",
      "processed 74000000 rows...\n",
      "processed 75000000 rows...\n",
      "processed 76000000 rows...\n",
      "processed 77000000 rows...\n",
      "processed 78000000 rows...\n",
      "processed 79000000 rows...\n",
      "processed 80000000 rows...\n",
      "processed 81000000 rows...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>anime_id</th>\n",
       "      <th>my_score</th>\n",
       "      <th>year_watched</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80000002</th>\n",
       "      <td>DittoGang</td>\n",
       "      <td>2129</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80000038</th>\n",
       "      <td>DittoGang</td>\n",
       "      <td>5671</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80000040</th>\n",
       "      <td>DittoGang</td>\n",
       "      <td>5909</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80000063</th>\n",
       "      <td>DittoGang</td>\n",
       "      <td>8525</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80000065</th>\n",
       "      <td>DittoGang</td>\n",
       "      <td>8675</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80000073</th>\n",
       "      <td>DittoGang</td>\n",
       "      <td>9055</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80000089</th>\n",
       "      <td>DittoGang</td>\n",
       "      <td>10080</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80000096</th>\n",
       "      <td>DittoGang</td>\n",
       "      <td>10232</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80000100</th>\n",
       "      <td>DittoGang</td>\n",
       "      <td>10464</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80000109</th>\n",
       "      <td>DittoGang</td>\n",
       "      <td>10884</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80000114</th>\n",
       "      <td>DittoGang</td>\n",
       "      <td>11227</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80000132</th>\n",
       "      <td>DittoGang</td>\n",
       "      <td>12175</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80000134</th>\n",
       "      <td>DittoGang</td>\n",
       "      <td>12281</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80000143</th>\n",
       "      <td>DittoGang</td>\n",
       "      <td>12863</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80000166</th>\n",
       "      <td>DittoGang</td>\n",
       "      <td>14813</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80000182</th>\n",
       "      <td>DittoGang</td>\n",
       "      <td>15731</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80000188</th>\n",
       "      <td>DittoGang</td>\n",
       "      <td>16123</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80000195</th>\n",
       "      <td>DittoGang</td>\n",
       "      <td>16706</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80000203</th>\n",
       "      <td>DittoGang</td>\n",
       "      <td>17549</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80000215</th>\n",
       "      <td>DittoGang</td>\n",
       "      <td>18119</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           username  anime_id  my_score  year_watched\n",
       "80000002  DittoGang      2129       8.0          2017\n",
       "80000038  DittoGang      5671       9.0          2017\n",
       "80000040  DittoGang      5909       7.0          2017\n",
       "80000063  DittoGang      8525       7.0          2017\n",
       "80000065  DittoGang      8675       8.0          2017\n",
       "80000073  DittoGang      9055       8.0          2018\n",
       "80000089  DittoGang     10080       7.0          2017\n",
       "80000096  DittoGang     10232       7.0          2018\n",
       "80000100  DittoGang     10464       7.0          2017\n",
       "80000109  DittoGang     10884       8.0          2017\n",
       "80000114  DittoGang     11227       8.0          2017\n",
       "80000132  DittoGang     12175       7.0          2017\n",
       "80000134  DittoGang     12281       8.0          2017\n",
       "80000143  DittoGang     12863       8.0          2018\n",
       "80000166  DittoGang     14813       8.0          2017\n",
       "80000182  DittoGang     15731       8.0          2018\n",
       "80000188  DittoGang     16123       8.0          2017\n",
       "80000195  DittoGang     16706       8.0          2017\n",
       "80000203  DittoGang     17549       8.0          2018\n",
       "80000215  DittoGang     18119       8.0          2018"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import libraries\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import geonamescache\n",
    "from collections import defaultdict\n",
    "import string\n",
    "\n",
    "# function definitions\n",
    "def calculate_age(born):\n",
    "    if type(born) != int:\n",
    "        try:\n",
    "            born = datetime.strptime(born, '%Y-%m-%d').date()\n",
    "            today = date.today()\n",
    "            if (today.month, today.year) < (born.month, born.year):\n",
    "                age = (today.year - born.year) - 1\n",
    "            else:\n",
    "                age = today.year - born.year\n",
    "\n",
    "            return age\n",
    "        except:\n",
    "            print('corrupt data')\n",
    "            return 0\n",
    "    \n",
    "# compare each element in 'user location' with each possible city or country\n",
    "# note: this method can be enhanced with regex\n",
    "\n",
    "# note: a 'states' database needs to be added\n",
    "# this would assign many users the USA\n",
    "\n",
    "# as of now, this method will only assign a user with a country if an exact\n",
    "# city/country name is specified by the user and that city/country exists in\n",
    "# the 'worldcities' csv data file\n",
    "def assign_country(location, user_row):\n",
    "    print('assigning country for row:', user_row)\n",
    "    \n",
    "    # if locatation was a replaced nan value, skip this row\n",
    "    if location == 0:\n",
    "        return\n",
    "    \n",
    "    for word in location:\n",
    "        for city_list in cities_df[['city']]:\n",
    "            city_obj = cities_df[city_list]\n",
    "            city_row = 0\n",
    "            for city in city_obj.values:\n",
    "                if word.strip() == city or \\\n",
    "                word.strip() == cities_df.at[city_row, 'country']:\n",
    "                    temp = []\n",
    "                    temp.append(cities_df.loc[city_row, 'country'])\n",
    "                    user_df.loc[user_row, 'location'] = temp[0]\n",
    "                    return\n",
    "                city_row += 1\n",
    "\n",
    "                \n",
    "# assign csv data urls\n",
    "# anime_data    = 'https://f000.backblazeb2.com/file/mal-db/AnimeList.csv'\n",
    "# user_data     = 'https://f000.backblazeb2.com/file/mal-db/UserList.csv'\n",
    "# user_mal_data = 'https://f000.backblazeb2.com/file/mal-db/UserAnimeList.csv'\n",
    "\n",
    "# local directory\n",
    "anime_data    = 'C:\\\\Users\\\\Uri\\\\Desktop\\\\data\\\\AnimeList.csv'\n",
    "user_data     = 'C:\\\\Users\\\\Uri\\\\Desktop\\\\data\\\\UserList.csv'\n",
    "user_mal_data = 'C:\\\\Users\\\\Uri\\\\Desktop\\\\data\\\\UserAnimeList.csv'\n",
    "\n",
    "cities = 'worldcities.csv'\n",
    "\n",
    "# set maximum number of rows to 20 & define NaN identifiers\n",
    "pd.set_option('max_rows', 20)\n",
    "idntfrs = ['na', '-', '--', '?', 'None', 'none', 'non', '', ' ', \\\n",
    "           'Not available', '0', '0000-00-00', 'NaT']\n",
    "\n",
    "# read & import data into pandas data frames\n",
    "anime_df    = pd.read_csv(anime_data, na_values=idntfrs)\n",
    "user_df     = pd.read_csv(user_data, na_values=idntfrs)\n",
    "cities_df   = pd.read_csv(cities)\n",
    "\n",
    "# my_reader = pd.read_csv(user_mal_data, chunksize=my_chunk, iterator=True)\n",
    "# user_mal_df = pd.concat(my_reader, ignore_index=True)\n",
    "\n",
    "# since UserAnimeList.csv is too large, read it from a generator in chunks\n",
    "# user_mal_gen = pd.read_csv(user_mal_data, na_values=idntfrs, iterator=True, \\\n",
    "#                            chunksize = my_chunk)\n",
    "# user_mal_df  = next(user_mal_gen)\n",
    "\n",
    "# read in user_data in small chunks for testing country methods...\n",
    "# my_chunk = 1000\n",
    "# user_gen = pd.read_csv(user_data, na_values=idntfrs, iterator=True, \\\n",
    "#                            chunksize = my_chunk)\n",
    "# user_df  = next(user_gen)\n",
    "\n",
    "\n",
    "# display shape and rows of each data frame\n",
    "# print('anime_df Shape:', anime_df.shape)\n",
    "# anime_df.head()\n",
    "\n",
    "# print('user_df Shape:', user_df.shape)\n",
    "# user_df.head()\n",
    "\n",
    "# print('user_mal_df Shape:', user_mal_df.shape)\n",
    "# user_mal_df.head()\n",
    "\n",
    "# drop unwanted features from the data frames\n",
    "anime_df.drop(['title_english', 'title_japanese', 'title_synonyms', \\\n",
    " \t\t\t   'image_url', 'type', 'source', 'episodes', 'airing', 'aired', \\\n",
    " \t\t\t   'duration', 'rating', 'broadcast', 'related', \\\n",
    " \t\t\t   'producer', 'licensor', 'premiered', 'studio', 'opening_theme', \\\n",
    " \t\t\t   'ending_theme', 'background', 'favorites'],\n",
    "               axis=1, inplace=True)\n",
    "\n",
    "user_df.drop(['user_watching', 'user_completed', 'user_onhold', 'user_dropped', \\\n",
    "              'user_plantowatch', 'user_days_spent_watching', 'access_rank', \\\n",
    "              'join_date', 'last_online', 'stats_mean_score', 'stats_rewatched', \\\n",
    "              'stats_episodes'],\n",
    "               axis=1, inplace=True)\n",
    "\n",
    "#user_df.drop(['location'], axis=1, inplace=True)\n",
    "\n",
    "# convert location to a list of lowercase, alphabetical strings\n",
    "user_df['location'] = user_df.location.str.lower()\n",
    "user_df['location'] = user_df.location.str.split(',')\n",
    "\n",
    "#delchars = string.punctuation\n",
    "# delchars = ''.join(c for c in map(chr, range(256)) if not c.isalpha())\n",
    "\n",
    "# user_df['location'] = user_df.location.str.translate(None, delchars)\n",
    "\n",
    "\n",
    "#user_df['location'] = ''.join(ch for ch in user_df.location.str if ch.isalpha())\n",
    "\n",
    "\n",
    "cities_df.drop(['city', 'lat', 'lng', 'iso2', 'iso3', \\\n",
    "                'admin_name', 'capital', 'population', 'id'],\n",
    "                 axis=1, inplace=True)\n",
    "cities_df.rename(columns={'city_ascii': 'city'}, inplace=True)\n",
    "cities_df['city'] = cities_df.city.str.lower()\n",
    "cities_df['country'] = cities_df.country.str.lower()\n",
    "\n",
    "\n",
    "# print('cities:')\n",
    "# for column in cities_df[['city']]:\n",
    "#     column_obj = cities_df[column]\n",
    "#     for city in column_obj.values:\n",
    "#         print(city)\n",
    "\n",
    "# print('locations:')\n",
    "# for column in user_df[['location']]:\n",
    "#     column_obj = user_df[column]\n",
    "#     column_obj.dropna(inplace=True)\n",
    "#     for location in column_obj.values:\n",
    "#         print(location)\n",
    "\n",
    "# user_df.dropna(inplace=True)\n",
    "# for i in range(0, 50):\n",
    "#      print(user_df.at[i, 'location'])\n",
    "\n",
    "\n",
    "\n",
    "# assign each user's location with a country\n",
    "# user_row = 0\n",
    "# user_df.fillna(0, inplace=True)\n",
    "# for column in user_df[['location']]:\n",
    "#     column_obj = user_df[column]\n",
    "#     column_obj.dropna(inplace=True)\n",
    "#     for location in column_obj.values:    # this loop takes a while...\n",
    "#         assign_country(location, user_row)\n",
    "#         user_row += 1\n",
    "# user_df.rename(columns={'location': 'country'}, inplace=True)\n",
    "# user_df['country'] = user_df.country.str.capitalize()\n",
    "\n",
    "# fix broken apostrophes across the entire dataframe\n",
    "# anime_df.replace('&#039;', '\\'', inplace=True)\n",
    "anime_df['title'] = anime_df['title'].str.replace('&#039;', '\\'')\n",
    "\n",
    "\n",
    "# clean birth_date column so that it represents age as a number\n",
    "# user_df['age'] = user_df['birth_date'].apply(calculate_age)\n",
    "# user_df.drop('birth_date', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# drop all N/A values from the data frames\n",
    "# anime_df.dropna(inplace=True)\n",
    "# user_df.dropna(inplace=True)\n",
    "# user_df['age'] = user_df['age'].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "# clean location column so that it only list country\n",
    "# gc = geonamescache.GeonamesCache()\n",
    "# countries = gc.get_countries()\n",
    "# print(countries)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# remove animes that have not yet aired since they don't have scoring data\n",
    "# anime_df = anime_df[~anime_df['status'].isin(['Not yet aired'])]\n",
    "\n",
    "# remove NSFW content\n",
    "# anime_df = anime_df[~anime_df['genre'].astype(str).str.contains('Hentai')]\n",
    "\n",
    "# convert genres to a list\n",
    "# anime_df['genre'] = anime_df.genre.str.split(',')\n",
    "\n",
    "# write cleaned data frames to csv files\n",
    "# anime_df.to_csv('anime.csv', index=False)\n",
    "# user_df.to_csv('user.csv', index=False)\n",
    "# user_df.to_csv('user_country_age.csv', index=False)\n",
    "\n",
    "#print('anime_df Shape:', anime_df.shape)\n",
    "#anime_df.head()\n",
    "#anime_df.isna().sum()\n",
    "\n",
    "#print('user_df Shape:', user_df.shape)\n",
    "#user_df.head()\n",
    "# user_df.isna().sum()\n",
    "\n",
    "# print('user_mal_df Shape:', user_mal_df.shape)\n",
    "# user_mal_df.head()\n",
    "# user_mal_df.isna().sum()\n",
    "\n",
    "# define chunk size for user_mal_data since the file is too large\n",
    "my_chunk = 10**6\n",
    "i = 0\n",
    "first_chunk = True\n",
    "for chunk in pd.read_csv(user_mal_data, na_values=idntfrs,\n",
    "                         chunksize=my_chunk, iterator=True):\n",
    "    user_mal_df = chunk\n",
    "    user_mal_df.drop(['my_watched_episodes', 'my_status', 'my_rewatching', \\\n",
    "                      'my_rewatching_ep', 'my_last_updated', 'my_tags', \\\n",
    "                      'my_finish_date'],\n",
    "                      axis=1, inplace=True)\n",
    "    user_mal_df.dropna(inplace=True)\n",
    "    \n",
    "#     # if a month or day is zero, set it to january 1st in order to not lose year\n",
    "#     row = 0\n",
    "#     for column in user_mal_df[['my_start_date']]:\n",
    "#         column_obj = user_mal_df[column]\n",
    "#         for date in column_obj.values:\n",
    "#             if date[6] == '0' or date[9] == '0':\n",
    "#                 s = list(date)\n",
    "#                 s[6] = '1'\n",
    "#                 s[9] = '1'\n",
    "#                 cdate = ''.join(s)\n",
    "#                 print(user_mal_df.loc[row, 'my_start_date'])\n",
    "#                 user_mal_df.loc[row, 'my_start_date'] = cdate\n",
    "#             row += 1\n",
    "#             print('fixed row #:', row)\n",
    "                \n",
    "    # convert start date to datetime object and extract date\n",
    "    user_mal_df['my_start_date'] = pd.to_datetime(user_mal_df['my_start_date'], errors='coerce')\n",
    "    user_mal_df['year_watched'] = user_mal_df['my_start_date'].dt.year\n",
    "    user_mal_df.dropna(inplace=True)\n",
    "    user_mal_df.drop('my_start_date', axis=1, inplace=True)\n",
    "    user_mal_df['year_watched'] = user_mal_df['year_watched'].astype(int)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if first_chunk:   \n",
    "        user_mal_df.to_csv('start_dates.csv', mode='w', header=True, index=False)\n",
    "        first_chunk = False\n",
    "    else:\n",
    "        user_mal_df.to_csv('start_dates.csv', mode='a', header=False, index=False)\n",
    "    \n",
    "    i += 1\n",
    "    print('processed', my_chunk*i, 'rows...')\n",
    "\n",
    "\n",
    "#print('cities_df Shape:', cities_df.shape)\n",
    "#cities_df.head()\n",
    "\n",
    "# note: the final user_df is pretty depricated because only one \n",
    "# database is being used for location. \n",
    "# less users will be depricated when more location databases are added...\n",
    "# user_df.head(100)\n",
    "user_mal_df.head(20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "anime_recommender.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ECE143",
   "language": "python",
   "name": "ece143"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
