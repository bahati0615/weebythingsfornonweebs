{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p8QRrhMnjKoH"
   },
   "source": [
    "From the 3 CSV files, we will begin cleaing and filtering the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "id": "_BzZFlG_ioBA",
    "outputId": "4f2552f1-ceea-471b-c2d3-462c8c3c9283"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assigning country for row: 0\n",
      "assigning country for row: 1\n",
      "assigning country for row: 2\n",
      "assigning country for row: 3\n",
      "assigning country for row: 4\n",
      "assigning country for row: 5\n",
      "assigning country for row: 6\n",
      "assigning country for row: 7\n",
      "assigning country for row: 8\n",
      "assigning country for row: 9\n",
      "assigning country for row: 10\n",
      "assigning country for row: 11\n",
      "assigning country for row: 12\n",
      "assigning country for row: 13\n",
      "assigning country for row: 14\n",
      "assigning country for row: 15\n",
      "assigning country for row: 16\n",
      "assigning country for row: 17\n",
      "assigning country for row: 18\n",
      "assigning country for row: 19\n",
      "assigning country for row: 20\n",
      "assigning country for row: 21\n",
      "assigning country for row: 22\n",
      "assigning country for row: 23\n",
      "assigning country for row: 24\n",
      "assigning country for row: 25\n",
      "assigning country for row: 26\n",
      "assigning country for row: 27\n",
      "assigning country for row: 28\n",
      "assigning country for row: 29\n",
      "assigning country for row: 30\n",
      "assigning country for row: 31\n",
      "assigning country for row: 32\n",
      "assigning country for row: 33\n",
      "assigning country for row: 34\n",
      "assigning country for row: 35\n",
      "assigning country for row: 36\n",
      "assigning country for row: 37\n",
      "assigning country for row: 38\n",
      "assigning country for row: 39\n",
      "assigning country for row: 40\n",
      "assigning country for row: 41\n",
      "assigning country for row: 42\n",
      "assigning country for row: 43\n",
      "assigning country for row: 44\n",
      "assigning country for row: 45\n",
      "assigning country for row: 46\n",
      "assigning country for row: 47\n",
      "assigning country for row: 48\n",
      "assigning country for row: 49\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>user_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>country</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>karthiga</td>\n",
       "      <td>2255153</td>\n",
       "      <td>Female</td>\n",
       "      <td>India</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RedvelvetDaisuki</td>\n",
       "      <td>1897606</td>\n",
       "      <td>Female</td>\n",
       "      <td>Philippines</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Damonashu</td>\n",
       "      <td>37326</td>\n",
       "      <td>Male</td>\n",
       "      <td>United states</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bskai</td>\n",
       "      <td>228342</td>\n",
       "      <td>Male</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>terune_uzumaki</td>\n",
       "      <td>327311</td>\n",
       "      <td>Female</td>\n",
       "      <td>Malaysia</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bas_G</td>\n",
       "      <td>5015094</td>\n",
       "      <td>Male</td>\n",
       "      <td>United states</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>HimeAria</td>\n",
       "      <td>3129315</td>\n",
       "      <td>Female</td>\n",
       "      <td>Poland</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Slimak</td>\n",
       "      <td>61677</td>\n",
       "      <td>Male</td>\n",
       "      <td>Poland</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>jimothy000</td>\n",
       "      <td>47167</td>\n",
       "      <td>Male</td>\n",
       "      <td>United kingdom</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ProperBritish</td>\n",
       "      <td>253613</td>\n",
       "      <td>Male</td>\n",
       "      <td>United kingdom</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Xinil</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>United states</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>helenply</td>\n",
       "      <td>2529849</td>\n",
       "      <td>Female</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>detestedlove311</td>\n",
       "      <td>5309</td>\n",
       "      <td>Female</td>\n",
       "      <td>United states</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>ihasabucket</td>\n",
       "      <td>18867</td>\n",
       "      <td>Male</td>\n",
       "      <td>United states</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>fluffylonewolf</td>\n",
       "      <td>1537661</td>\n",
       "      <td>Female</td>\n",
       "      <td>Canada</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Lithuelle</td>\n",
       "      <td>2637159</td>\n",
       "      <td>Female</td>\n",
       "      <td>Germany</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            username  user_id  gender         country  age\n",
       "0           karthiga  2255153  Female           India   30\n",
       "1   RedvelvetDaisuki  1897606  Female     Philippines   26\n",
       "2          Damonashu    37326    Male   United states   29\n",
       "3              bskai   228342    Male          Mexico   30\n",
       "5     terune_uzumaki   327311  Female        Malaysia   22\n",
       "6              Bas_G  5015094    Male   United states   21\n",
       "12          HimeAria  3129315  Female          Poland   24\n",
       "14            Slimak    61677    Male          Poland   33\n",
       "16        jimothy000    47167    Male  United kingdom   25\n",
       "20     ProperBritish   253613    Male  United kingdom   29\n",
       "25             Xinil        1    Male   United states   35\n",
       "27          helenply  2529849  Female          Mexico   21\n",
       "31   detestedlove311     5309  Female   United states   31\n",
       "32       ihasabucket    18867    Male   United states   36\n",
       "43    fluffylonewolf  1537661  Female          Canada   32\n",
       "44         Lithuelle  2637159  Female         Germany   35"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import libraries\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import geonamescache\n",
    "from collections import defaultdict\n",
    "import string\n",
    "\n",
    "# function definitions\n",
    "def calculate_age(born):\n",
    "    if type(born) != int:\n",
    "        born = datetime.strptime(born, '%Y-%m-%d').date()\n",
    "        today = date.today()\n",
    "        if (today.month, today.year) < (born.month, born.year):\n",
    "            age = (today.year - born.year) - 1\n",
    "        else:\n",
    "            age = today.year - born.year\n",
    "    \n",
    "        return age\n",
    "    \n",
    "# compare each element in 'user location' with each possible city or country\n",
    "# note: this method can be enhanced with regex\n",
    "\n",
    "# note: a 'states' database needs to be added\n",
    "# this would assign many users the USA\n",
    "\n",
    "# as of now, this method will only assign a user with a country if an exact\n",
    "# city/country name is specified by the user and that city/country exists in\n",
    "# the 'worldcities' csv data file\n",
    "def assign_country(location, user_row):\n",
    "    print('assigning country for row:', user_row)\n",
    "    \n",
    "    # if locatation was a replaced nan value, skip this row\n",
    "    if location == 0:\n",
    "        return\n",
    "    \n",
    "    for word in location:\n",
    "        for city_list in cities_df[['city']]:\n",
    "            city_obj = cities_df[city_list]\n",
    "            city_row = 0\n",
    "            for city in city_obj.values:\n",
    "                if word.strip() == city or \\\n",
    "                word.strip() == cities_df.at[city_row, 'country']:\n",
    "                    temp = []\n",
    "                    temp.append(cities_df.loc[city_row, 'country'])\n",
    "                    user_df.loc[user_row, 'location'] = temp[0]\n",
    "                    return\n",
    "                city_row += 1\n",
    "\n",
    "                \n",
    "# assign csv data urls\n",
    "# anime_data    = 'https://f000.backblazeb2.com/file/mal-db/AnimeList.csv'\n",
    "# user_data     = 'https://f000.backblazeb2.com/file/mal-db/UserList.csv'\n",
    "# user_mal_data = 'https://f000.backblazeb2.com/file/mal-db/UserAnimeList.csv'\n",
    "\n",
    "# local directory\n",
    "anime_data    = 'C:\\\\Users\\\\Uri\\\\Desktop\\\\data\\\\AnimeList.csv'\n",
    "user_data     = 'C:\\\\Users\\\\Uri\\\\Desktop\\\\data\\\\UserList.csv'\n",
    "user_mal_data = 'C:\\\\Users\\\\Uri\\\\Desktop\\\\data\\\\UserAnimeList.csv'\n",
    "\n",
    "cities = 'worldcities.csv'\n",
    "\n",
    "# set maximum number of rows to 20 & define NaN identifiers\n",
    "pd.set_option('max_rows', 20)\n",
    "idntfrs = ['na', '-', '--', '?', 'None', 'none', 'non', '', ' ', \\\n",
    "           'Not available', '0']\n",
    "\n",
    "# read & import data into pandas data frames\n",
    "anime_df    = pd.read_csv(anime_data, na_values=idntfrs)\n",
    "#user_df     = pd.read_csv(user_data, na_values=idntfrs)\n",
    "cities_df   = pd.read_csv(cities)\n",
    "\n",
    "# my_reader = pd.read_csv(user_mal_data, chunksize=my_chunk, iterator=True)\n",
    "# user_mal_df = pd.concat(my_reader, ignore_index=True)\n",
    "\n",
    "# since UserAnimeList.csv is too large, read it from a generator in chunks\n",
    "# user_mal_gen = pd.read_csv(user_mal_data, na_values=idntfrs, iterator=True, \\\n",
    "#                            chunksize = my_chunk)\n",
    "# user_mal_df  = next(user_mal_gen)\n",
    "\n",
    "# read in user_data in small chunks for testing country methods...\n",
    "my_chunk = 50\n",
    "user_gen = pd.read_csv(user_data, na_values=idntfrs, iterator=True, \\\n",
    "                           chunksize = my_chunk)\n",
    "user_df  = next(user_gen)\n",
    "\n",
    "\n",
    "# display shape and rows of each data frame\n",
    "# print('anime_df Shape:', anime_df.shape)\n",
    "# anime_df.head()\n",
    "\n",
    "# print('user_df Shape:', user_df.shape)\n",
    "# user_df.head()\n",
    "\n",
    "# print('user_mal_df Shape:', user_mal_df.shape)\n",
    "# user_mal_df.head()\n",
    "\n",
    "# drop unwanted features from the data frames\n",
    "anime_df.drop(['title_english', 'title_japanese', 'title_synonyms', \\\n",
    " \t\t\t   'image_url', 'type', 'source', 'episodes', 'airing', 'aired', \\\n",
    " \t\t\t   'duration', 'rating', 'broadcast', 'related', \\\n",
    " \t\t\t   'producer', 'licensor', 'premiered', 'studio', 'opening_theme', \\\n",
    " \t\t\t   'ending_theme', 'background', 'favorites'],\n",
    "               axis=1, inplace=True)\n",
    "\n",
    "user_df.drop(['user_watching', 'user_completed', 'user_onhold', 'user_dropped', \\\n",
    "              'user_plantowatch', 'user_days_spent_watching', 'access_rank', \\\n",
    "              'join_date', 'last_online', 'stats_mean_score', 'stats_rewatched', \\\n",
    "              'stats_episodes'],\n",
    "               axis=1, inplace=True)\n",
    "\n",
    "# convert location to a list of lowercase, alphabetical strings\n",
    "user_df['location'] = user_df.location.str.lower()\n",
    "user_df['location'] = user_df.location.str.split(',')\n",
    "\n",
    "#delchars = string.punctuation\n",
    "# delchars = ''.join(c for c in map(chr, range(256)) if not c.isalpha())\n",
    "\n",
    "# user_df['location'] = user_df.location.str.translate(None, delchars)\n",
    "\n",
    "\n",
    "#user_df['location'] = ''.join(ch for ch in user_df.location.str if ch.isalpha())\n",
    "\n",
    "\n",
    "cities_df.drop(['city', 'lat', 'lng', 'iso2', 'iso3', \\\n",
    "                'admin_name', 'capital', 'population', 'id'],\n",
    "                 axis=1, inplace=True)\n",
    "cities_df.rename(columns={'city_ascii': 'city'}, inplace=True)\n",
    "cities_df['city'] = cities_df.city.str.lower()\n",
    "cities_df['country'] = cities_df.country.str.lower()\n",
    "\n",
    "\n",
    "# print('cities:')\n",
    "# for column in cities_df[['city']]:\n",
    "#     column_obj = cities_df[column]\n",
    "#     for city in column_obj.values:\n",
    "#         print(city)\n",
    "\n",
    "# print('locations:')\n",
    "# for column in user_df[['location']]:\n",
    "#     column_obj = user_df[column]\n",
    "#     column_obj.dropna(inplace=True)\n",
    "#     for location in column_obj.values:\n",
    "#         print(location)\n",
    "\n",
    "# user_df.dropna(inplace=True)\n",
    "# for i in range(0, 50):\n",
    "#      print(user_df.at[i, 'location'])\n",
    "\n",
    "\n",
    "\n",
    "# assign each user's location with a country\n",
    "user_row = 0\n",
    "user_df.fillna(0, inplace=True)\n",
    "for column in user_df[['location']]:\n",
    "    column_obj = user_df[column]\n",
    "    column_obj.dropna(inplace=True)\n",
    "    for location in column_obj.values:    # this loop takes a while...\n",
    "        assign_country(location, user_row)\n",
    "        user_row += 1\n",
    "user_df.rename(columns={'location': 'country'}, inplace=True)\n",
    "user_df['country'] = user_df.country.str.capitalize()\n",
    "\n",
    "# clean birth_date column so that it represents age as a number\n",
    "user_df['age'] = user_df['birth_date'].apply(calculate_age)\n",
    "user_df.drop('birth_date', axis=1, inplace=True)\n",
    "\n",
    "# drop all N/A values from the data frames\n",
    "anime_df.dropna(inplace=True)\n",
    "user_df.dropna(inplace=True)\n",
    "\n",
    "# fix broken apostrophes across the entire dataframe\n",
    "# anime_df.replace('&#039;', '\\'', inplace=True)\n",
    "anime_df['title'] = anime_df['title'].str.replace('&#039;', '\\'')\n",
    "user_df['age'] = user_df['age'].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# clean location column so that it only list country\n",
    "# gc = geonamescache.GeonamesCache()\n",
    "# countries = gc.get_countries()\n",
    "# print(countries)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# remove animes that have not yet aired since they don't have scoring data\n",
    "# anime_df = anime_df[~anime_df['status'].isin(['Not yet aired'])]\n",
    "\n",
    "# remove NSFW content\n",
    "# anime_df = anime_df[~anime_df['genre'].astype(str).str.contains('Hentai')]\n",
    "\n",
    "# convert genres to a list\n",
    "# anime_df['genre'] = anime_df.genre.str.split(',')\n",
    "\n",
    "# write cleaned data frames to csv files\n",
    "# anime_df.to_csv('anime.csv', index=False)\n",
    "# user_df.to_csv('user.csv', index=False)\n",
    "user_df.to_csv('user_country_age.csv', index=False)\n",
    "\n",
    "#print('anime_df Shape:', anime_df.shape)\n",
    "#anime_df.head()\n",
    "#anime_df.isna().sum()\n",
    "\n",
    "#print('user_df Shape:', user_df.shape)\n",
    "#user_df.head()\n",
    "# user_df.isna().sum()\n",
    "\n",
    "# print('user_mal_df Shape:', user_mal_df.shape)\n",
    "# user_mal_df.head()\n",
    "# user_mal_df.isna().sum()\n",
    "\n",
    "# define chunk size for user_mal_data since the file is too large\n",
    "# my_chunk = 10**5\n",
    "# first_chunk = True\n",
    "# for chunk in pd.read_csv(user_mal_data, na_values=idntfrs,\n",
    "#                          chunksize=my_chunk, iterator=True):\n",
    "#     user_mal_df = chunk\n",
    "#     user_mal_df.drop(['my_watched_episodes', 'my_status', 'my_rewatching', \\\n",
    "#                       'my_rewatching_ep', 'my_last_updated', 'my_tags', \\\n",
    "#                       'my_start_date', 'my_finish_date'],\n",
    "#                       axis=1, inplace=True)\n",
    "#     user_mal_df.dropna(inplace=True)\n",
    "#     if first_chunk:   \n",
    "#         user_mal_df.to_csv('user_mal.csv', mode='w', header=True, index=False)\n",
    "#         first_chunk = False\n",
    "#     else:\n",
    "#         user_mal_df.to_csv('user_mal.csv', mode='a', header=False, index=False)\n",
    "\n",
    "\n",
    "#print('cities_df Shape:', cities_df.shape)\n",
    "#cities_df.head()\n",
    "\n",
    "# note: the final user_df is pretty depricated because only one \n",
    "# database is being used for location. \n",
    "# less users will be depricated when more location databases are added...\n",
    "user_df.head(20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "anime_recommender.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ECE143",
   "language": "python",
   "name": "ece143"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
